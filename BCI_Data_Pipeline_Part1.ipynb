{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz7KX5Y8TsEF",
        "outputId": "89ef722e-d3d0-4a62-eafc-6a0afd07a9ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision mat73 pymatreader matplotlib tensorboard mne numpy scipy numba scikit-learn PyWavelets pandas mne-features"
      ],
      "metadata": {
        "id": "IwcYMNsNT5KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io as scipy\n",
        "import string\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import zipfile\n",
        "import io\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import mat73\n",
        "import mne\n",
        "from enum import Enum\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import sys\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import mne_features\n",
        "import joblib"
      ],
      "metadata": {
        "id": "uwLntbBiT7Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title General File Operations\n",
        "def file_saving(current_dir, filenames, tensor_list, dir_name = None, key=None, feature=None):\n",
        "  assert len(filenames) == len(tensor_list)\n",
        "  if key is None:\n",
        "    print(f'Saving EEG and ECoG Tensors....',end='')\n",
        "  elif feature is None:\n",
        "    print(f'Saving {key} Tensors....',end='')\n",
        "  else:\n",
        "    assert key is not None and feature is not None\n",
        "    print(f'Saving {key}_{feature} Tensor(s)....')\n",
        "  checkpoint_dir = current_dir\n",
        "  if dir_name is not None:\n",
        "    checkpoint_dir = current_dir + '/'+dir_name\n",
        "  if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "  for ind in range(len(filenames)):\n",
        "    if type(tensor_list[ind]) is list:\n",
        "      tensor_shapes = [item.shape for item in tensor_list[ind]]\n",
        "    else:\n",
        "      tensor_shapes = tensor_list[ind].shape\n",
        "    print(f'\\tFile {filenames[ind]} shape ={tensor_shapes}')\n",
        "    file_loc = os.path.join(checkpoint_dir,filenames[ind])\n",
        "    torch.save(tensor_list[ind],file_loc)\n",
        "  print(f'Finished')\n",
        "\n",
        "                                #|     dataset_dirs         |        dataset_names              |\n",
        "                                #checkpoint4/normalized_data/powerset,zeroset,raw_dataset,meanset\n",
        "def save_datasets(current_dir, dataset_dirs, feature_names, tensor_list, key, specific_index = -1):\n",
        "  inner_directory = current_dir\n",
        "  #create the innermost directory\n",
        "  for dir_name in dataset_dirs:\n",
        "    inner_directory = os.path.join(inner_directory,dir_name)\n",
        "    if not os.path.exists(inner_directory):\n",
        "      os.makedirs(inner_directory)\n",
        "  assert len(feature_names) == len(tensor_list) # dataset_names = [data,power,mean,zero]; elem => tensor_list[elem]\n",
        "  dataset_type = ['train','validation','test'] # dataset_types; elem => tensor_list[i][elem]\n",
        "  for index in range(len(feature_names)):\n",
        "    file_names = []\n",
        "    if specific_index < 0:\n",
        "      for dataset_ind in range(len(dataset_type)):\n",
        "        file_names.append(f\"{key}_{feature_names[index]}_{dataset_type[dataset_ind]}.pt\")\n",
        "      assert len(file_names) == 3\n",
        "      print(f'tensor_list[index] length = {len(tensor_list[index])}')\n",
        "      file_saving(inner_directory,file_names,tensor_list[index],dir_name=feature_names[index]+\"_dir\")\n",
        "    else:\n",
        "      file_names = [f'{key}_{dataset_name}_{dataset_type[specific_index]}.pt' for dataset_name in feature_names]\n",
        "      to_save_tensors = [tensor[specific_index] for tensor in tensor_list]\n",
        "      file_saving(current_dir,file_names,to_save_tensors,dir_name=f'{key}_{dataset_type[specific_index]}')"
      ],
      "metadata": {
        "id": "2RLHot3sUAmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title File Extraction Driver\n",
        "\n",
        "\"\"\"\n",
        "To find the key in a loaded_mat(.mat file which returns the map)\n",
        "\"\"\"\n",
        "def find_key(diction, key_tag):\n",
        "  for key in diction:\n",
        "      if key_tag in key:\n",
        "        return key\n",
        "  return \"WaveData\" \n",
        "\n",
        "\"\"\"\n",
        "Smaller helper method to sort the ECoG and EEG file names(helps for numerical ordering)\n",
        "\"\"\"\n",
        "def separated_data_files(fileList):\n",
        "  eegfiles = []\n",
        "  ecogfiles = []\n",
        "  for elem in fileList :\n",
        "    child_elem = elem.split(\"/\")[-1]\n",
        "    if (\"EEG\" in child_elem):\n",
        "      eegfiles.append(elem)\n",
        "    elif (\"ECoG\" in child_elem):\n",
        "      ecogfiles.append(elem)\n",
        "  \n",
        "  assert len(eegfiles) == len(ecogfiles)\n",
        "  #align input with target\n",
        "  eegfiles.sort()\n",
        "  ecogfiles.sort()\n",
        "  return eegfiles, ecogfiles\n",
        "\n",
        "\"\"\"\n",
        "Method extracts the data from the file\n",
        "\"\"\"\n",
        "def extract_file_data(data_dir,file_name, data_key,zipObj):\n",
        "  extracted_file = os.path.join(data_dir,file_name)\n",
        "  if not os.path.exists(extracted_file):\n",
        "    print(f'\\t\\tExtracting {file_name}....',end='')\n",
        "    extracted_file = zipObj.extract(file_name,path=data_dir)\n",
        "  if \"EEG\" in data_key:\n",
        "    loaded_mat = scipy.loadmat(extracted_file)\n",
        "  elif \"ECoG\" in data_key:\n",
        "    loaded_mat = mat73.loadmat(extracted_file)\n",
        "  keyname = find_key(loaded_mat,data_key)\n",
        "  final_form = torch.tensor(loaded_mat[keyname])\n",
        "  print(f'Finished Extracting {file_name}_{data_key}: Key={keyname}\\tShape={final_form.shape}')\n",
        "  return final_form\n",
        "  \n",
        "\"\"\"\n",
        "A 'Top Level' method for extracting a specified directory\n",
        "\"\"\"\n",
        "def extract_directory(location):\n",
        "  print(f\"\\tExtracting files in {location}...\")\n",
        "  data_directory = location[:-4]+ \"_Extracted_Data\" # exlucde the .zip and append Extracted_Data\n",
        "  if not os.path.exists(data_directory):\n",
        "    os.makedirs(data_directory)\n",
        "  EEG_dir_Tensors = []\n",
        "  ECoG_dir_Tensors = []\n",
        "  with ZipFile(location, 'r') as zipObj :\n",
        "    # get sorted list of eeg and ecog files\n",
        "    fileList = zipObj.namelist() \n",
        "    eeg_files, ecog_files = separated_data_files(fileList)\n",
        "    assert len(eeg_files) == len(ecog_files)\n",
        "    for pair in range(len(eeg_files)):\n",
        "      EEG_dir_Tensors.append(extract_file_data(data_directory,eeg_files[pair],\"EEG\",zipObj))\n",
        "      ECoG_dir_Tensors.append(extract_file_data(data_directory,ecog_files[pair],\"ECoG\",zipObj))\n",
        "  print(f\"Finished Extracting\\n\")\n",
        "  return EEG_dir_Tensors, ECoG_dir_Tensors\n",
        "\n",
        "\"\"\"\n",
        "The Driver method for extracting data from the Zip files\n",
        "\"\"\"\n",
        "def extraction_driver(current_directory,zipnames, checkpoint = False):\n",
        "  EEG_Tensors = []\n",
        "  ECoG_Tensors = []\n",
        "  print('Beginning Extraction Process....')\n",
        "  for zipdir in zipnames:\n",
        "    file_loc = current_directory + \"/\" + zipdir\n",
        "    Dir_EEG_Tensors, Dir_ECoG_Tensors=extract_directory(file_loc)\n",
        "    # add the returned lists to the back of continued list\n",
        "    EEG_Tensors = EEG_Tensors + Dir_EEG_Tensors\n",
        "    ECoG_Tensors = ECoG_Tensors + Dir_ECoG_Tensors\n",
        "  print(f'Finished Extracting {len(EEG_Tensors)} Sequences')\n",
        "\n",
        "  # if user wants to save checkpoint\n",
        "  if checkpoint:\n",
        "    file_saving(current_directory,[\"raw_unfiltered_ecog_list.pt\",\"raw_unfiltered_eeg_list.pt\"],[ECoG_Tensors, EEG_Tensors],dir_name=\"checkpoint1\")\n",
        "\n",
        "  return EEG_Tensors, ECoG_Tensors"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1irvj0hPaG-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_eeg, extracted_ecog = extraction_driver(\"/content/gdrive/MyDrive/BCI_Project/Datasets\", [\"S1_EEGandECoG.zip\",\"S2_EEGandECoG.zip\",\"S3_EEGandECoG.zip\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75TTxg-UUW1h",
        "outputId": "5a01f2a0-7af4-4275-9268-e7640fa81161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning Extraction Process....\n",
            "\tExtracting files in /content/gdrive/MyDrive/BCI_Project/Datasets/S1_EEGandECoG.zip...\n",
            "Finished Extracting 20110607S1_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/EEG01.mat_EEG: Key=EEG2\tShape=torch.Size([19, 327815])\n",
            "Finished Extracting 20110607S1_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/ECoG01.mat_ECoG: Key=WaveData\tShape=torch.Size([129, 318156])\n",
            "Finished Extracting 20110607S1_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/EEG02.mat_EEG: Key=EEG2\tShape=torch.Size([19, 326744])\n",
            "Finished Extracting 20110607S1_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/ECoG02.mat_ECoG: Key=WaveData\tShape=torch.Size([129, 317907])\n",
            "Finished Extracting\n",
            "\n",
            "\tExtracting files in /content/gdrive/MyDrive/BCI_Project/Datasets/S2_EEGandECoG.zip...\n",
            "Finished Extracting 20110607S2_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/EEG01_anesthesia.mat_EEG: Key=EEG2\tShape=torch.Size([19, 331832])\n",
            "Finished Extracting 20110607S2_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/ECoG01_anesthesia.mat_ECoG: Key=WaveData\tShape=torch.Size([129, 353673])\n",
            "Finished Extracting 20110607S2_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/EEG02_anesthesia.mat_EEG: Key=EEG2\tShape=torch.Size([19, 335582])\n",
            "Finished Extracting 20110607S2_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/ECoG02_anesthesia.mat_ECoG: Key=WaveData\tShape=torch.Size([129, 332753])\n",
            "Finished Extracting 20110607S2_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/EEG03_anesthesia.mat_EEG: Key=EEG2\tShape=torch.Size([19, 327011])\n",
            "Finished Extracting 20110607S2_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/ECoG03_anesthesia.mat_ECoG: Key=WaveData\tShape=torch.Size([129, 316689])\n",
            "Finished Extracting 20110607S2_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/EEG04_anesthesia.mat_EEG: Key=EEG2\tShape=torch.Size([19, 338528])\n",
            "Finished Extracting 20110607S2_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/ECoG04_anesthesia.mat_ECoG: Key=WaveData\tShape=torch.Size([129, 323564])\n",
            "Finished Extracting 20110607S2_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/EEG05_anesthesia.mat_EEG: Key=EEG2\tShape=torch.Size([19, 323262])\n",
            "Finished Extracting 20110607S2_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/ECoG05_anesthesia.mat_ECoG: Key=WaveData\tShape=torch.Size([129, 319234])\n",
            "Finished Extracting\n",
            "\n",
            "\tExtracting files in /content/gdrive/MyDrive/BCI_Project/Datasets/S3_EEGandECoG.zip...\n",
            "Finished Extracting 20110607S3_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/EEG06_anesthesia.mat_EEG: Key=EEG2\tShape=torch.Size([19, 625901])\n",
            "Finished Extracting 20110607S3_EEGECoG_Su_Oosugi-Naoya+Nagasaka-Yasuo+Hasegawa+Naomi_ECoG128-EEG18_mat/ECoG06_anesthesia.mat_ECoG: Key=WaveData\tShape=torch.Size([129, 615952])\n",
            "Finished Extracting\n",
            "\n",
            "Finished Extracting 8 Sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Resampling Data Driver\n",
        "def calc_frequency(old_points, new_points,orig_frequency):\n",
        "  total_time = old_points/orig_frequency\n",
        "  new_freq = (new_points/total_time)\n",
        "\n",
        "  if(old_points > new_points):\n",
        "      return orig_frequency / new_freq, new_freq\n",
        "  else:\n",
        "    return new_freq/orig_frequency, new_freq\n",
        "\n",
        "def resample_eeg(eeg_tensor,ecog_seq_len,original_frequency, id, mode = \"ndarray\"):\n",
        "  eeg_data = eeg_tensor.data.numpy()\n",
        "  eeg_seq_len = len(eeg_data[1])\n",
        "  frequency_factor, new_frequency = calc_frequency(eeg_seq_len,ecog_seq_len,original_frequency)\n",
        "  print(f'\\tResampling Sequence {id}: {original_frequency} => {new_frequency}')\n",
        "  if mode in \"ndarray\":\n",
        "    if eeg_seq_len > ecog_seq_len : #down-sample branch\n",
        "      return torch.tensor(mne.filter.resample(eeg_data,down = frequency_factor, pad=\"constant\")), new_frequency\n",
        "    elif eeg_seq_len < ecog_seq_len: # up-sample branch\n",
        "      return torch.tensor(mne.filter.resample(eeg_data,up = frequency_factor,pad=\"constant\")), new_frequency\n",
        "  elif mode in \"raw\":\n",
        "    eeg_raw_array = mne.io.RawArray(eeg_data,mne.create_info(ch_names=len(eeg_data),sfreq=1000.0,ch_types='eeg',verbose=None))\n",
        "    eeg_data2,times = eeg_raw_array.resample(new_frequency)[:]\n",
        "    return torch.tensor(eeg_data2), new_frequency\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def resampling_driver(current_directory,eeg_data, ecog_lens, eeg_freqs, overwrite_unfiltered = False):\n",
        "  is_freq_list = type(eeg_freqs) is list\n",
        "  if is_freq_list:\n",
        "    assert len(eeg_freqs) == len(eeg_data)\n",
        "  assert len(ecog_lens) == len(eeg_data)\n",
        "\n",
        "  resampled_eeg_list = []\n",
        "  frequency_list = []\n",
        "  print('Beginning Resampling Process....')\n",
        "  #go through eegs\n",
        "  for tensor_id in range(len(eeg_data)):\n",
        "    if eeg_data[tensor_id].size(1) != ecog_lens[tensor_id]:\n",
        "      res = resample_eeg(eeg_data[tensor_id], ecog_lens[tensor_id],eeg_freqs[tensor_id] if is_freq_list else eeg_freqs, tensor_id)\n",
        "      resampled_eeg_list.append(res[0])\n",
        "      frequency_list.append(res[1])\n",
        "    else:\n",
        "      resampled_eeg_list.append(eeg_data[tensor_id])\n",
        "      frequency_list.append(eeg_freqs)\n",
        "  if overwrite_unfiltered:\n",
        "    print('Overwriting Unfiltered EEG Data with Resampled EEG Data....',end='')\n",
        "    checkpoint1_dir = current_directory + '/checkpoint1_extraction'\n",
        "    if not os.path.exists(checkpoint1_dir):\n",
        "      print(\"\\tUnable to save data as the checkpoint1_extraction step is not saved\")\n",
        "    else:\n",
        "      file_saving(current_directory,[\"resampled_unfiltered_eeg_tensor_list.pt\"],[resampled_eeg_list],dir_name=\"checkpoint1\")\n",
        "  return resampled_eeg_list, frequency_list"
      ],
      "metadata": {
        "cellView": "form",
        "id": "P9BjZatqWOsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ecog_seq_lens = [ecog_tens.size(1) for ecog_tens in extracted_ecog]\n",
        "resampled_eeg_tensors, frequency_tensors = resampling_driver(\"/content/gdrive/MyDrive/BCI_Project/Datasets\",extracted_eeg,ecog_seq_lens,1000.0,overwrite_unfiltered=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrpLZ3uVa48Q",
        "outputId": "58839ba1-eb81-4be1-aaec-325e538fb89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning Resampling Process....\n",
            "\tResampling Sequence 0: 1000.0 => 970.5352104083096\n",
            "\tResampling Sequence 1: 1000.0 => 972.9543618245476\n",
            "\tResampling Sequence 2: 1000.0 => 1065.819450806432\n",
            "\tResampling Sequence 3: 1000.0 => 991.5698696592785\n",
            "\tResampling Sequence 4: 1000.0 => 968.4353125735831\n",
            "\tResampling Sequence 5: 1000.0 => 955.7968617071556\n",
            "\tResampling Sequence 6: 1000.0 => 987.5395190279093\n",
            "\tResampling Sequence 7: 1000.0 => 984.1045149312752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(frequency_tensors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WChvYaqq7A77",
        "outputId": "609b22b9-cad6-46a6-cf92-2fc277d6f0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[970.5352104083096, 972.9543618245476, 1065.819450806432, 991.5698696592785, 968.4353125735831, 955.7968617071556, 987.5395190279093, 984.1045149312752]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Truncated, Spatially Filtered Sequences\n",
        "\n",
        "def find_minlen(tensor_list):\n",
        "  min_number =sys.maxsize\n",
        "  for item in tensor_list:\n",
        "    min_number = min(item.size(1),min_number)\n",
        "  return min_number\n",
        "\n",
        "\"\"\"\n",
        "Responsible for truncating and filtering channels out\n",
        "\"\"\"\n",
        "def truncate_tensors(tensor_list, min_len, key, specific_channels = None):\n",
        "  truncated_tensor_list = []\n",
        "  print(f'Beginning Trunc_Process for {key}\\n')\n",
        "  for ind in range(len(tensor_list)):\n",
        "    print(f'\\tProc. Sequence {ind}...',end='')\n",
        "\n",
        "    #for EEG, which may require left channels only\n",
        "    if specific_channels is not None:\n",
        "      truncated_tensor = tensor_list[ind].data[specific_channels,0:min_len]\n",
        "      print(f'Filtered Channels...',end='')\n",
        "    else:\n",
        "      truncated_tensor = tensor_list[ind].data[:,0:min_len]\n",
        "\n",
        "    truncated_tensor = CAR_signal_removal(truncated_tensor)\n",
        "    truncated_tensor_list.append(truncated_tensor)\n",
        "    print(f\"Shape={truncated_tensor_list[-1].shape}\")\n",
        "  \n",
        "  print('Combining Trunc_Sequences...',end='')\n",
        "  stacked_tensor = torch.stack(truncated_tensor_list)\n",
        "  print(f'Combined {key} Shape={stacked_tensor.shape}\\n')\n",
        "  return stacked_tensor\n",
        "\n",
        "\"\"\"\n",
        "Responsible for spatial filtering and transposing input tensor\n",
        "\"\"\"\n",
        "def CAR_signal_removal(tensor):\n",
        "  mean_tensor = torch.mean(tensor,dim=0)\n",
        "  print(f'Removing CAR({mean_tensor.shape})...',end='')\n",
        "  assert mean_tensor.size(0) == tensor.size(1)\n",
        "  #remove CAR from each channel\n",
        "  for row in range(tensor.size(0)):\n",
        "    tensor[row] -= mean_tensor\n",
        "  print(f'Transposed...Finished')\n",
        "  return torch.transpose(tensor,0,1)\n",
        "\n",
        "def truncate_driver(current_directory,eeg_tensors, ecog_tensors, eeg_filtered_channels = None, save_checkpoint2=False):\n",
        "  assert len(eeg_tensors) == len(ecog_tensors)\n",
        "  min_len_ecog = find_minlen(ecog_tensors)\n",
        "  min_len_eeg = find_minlen(eeg_tensors)\n",
        "  assert min_len_eeg == min_len_ecog\n",
        "  cleaned_eeg_tensor = truncate_tensors(eeg_tensors,min_len_ecog,\"EEG\",specific_channels=eeg_filtered_channels)\n",
        "  cleaned_ecog_tensor = truncate_tensors(ecog_tensors,min_len_ecog,\"ECoG\")\n",
        "  assert cleaned_eeg_tensor.shape[0:1] == cleaned_ecog_tensor.shape[0:1]\n",
        "  if save_checkpoint2:\n",
        "    eeg_file_name = \"cleaned_eeg_data_tensor.pt\" if eeg_filtered_channels is None else \"cleaned_LEFT_eeg_data_tensor.pt\"\n",
        "    file_saving(current_directory,[eeg_file_name,\"cleaned_ecog_data_tensor.pt\"],[cleaned_eeg_tensor,cleaned_ecog_tensor],dir_name=\"checkpoint2\")\n",
        "  return cleaned_eeg_tensor, cleaned_ecog_tensor"
      ],
      "metadata": {
        "id": "IqSfukK9ckh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_eeg, cleaned_ecog = truncate_driver(\"/content/gdrive/MyDrive/BCI_Project/Datasets\",resampled_eeg_tensors,extracted_ecog)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFD-6c_IXF28",
        "outputId": "248ceff5-a3cd-49bf-9c86-de3ea171e536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning Trunc_Process for EEG\n",
            "\n",
            "\tProc. Sequence 0...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 19])\n",
            "\tProc. Sequence 1...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 19])\n",
            "\tProc. Sequence 2...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 19])\n",
            "\tProc. Sequence 3...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 19])\n",
            "\tProc. Sequence 4...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 19])\n",
            "\tProc. Sequence 5...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 19])\n",
            "\tProc. Sequence 6...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 19])\n",
            "\tProc. Sequence 7...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 19])\n",
            "Combining Trunc_Sequences...Combined EEG Shape=torch.Size([8, 316689, 19])\n",
            "\n",
            "Beginning Trunc_Process for ECoG\n",
            "\n",
            "\tProc. Sequence 0...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 129])\n",
            "\tProc. Sequence 1...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 129])\n",
            "\tProc. Sequence 2...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 129])\n",
            "\tProc. Sequence 3...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 129])\n",
            "\tProc. Sequence 4...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 129])\n",
            "\tProc. Sequence 5...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 129])\n",
            "\tProc. Sequence 6...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 129])\n",
            "\tProc. Sequence 7...Removing CAR(torch.Size([316689]))...Transposed...Finished\n",
            "Shape=torch.Size([316689, 129])\n",
            "Combining Trunc_Sequences...Combined ECoG Shape=torch.Size([8, 316689, 129])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Feature Extraction\n",
        "import numpy as np\n",
        "import torch\n",
        "import mne_features\n",
        "\n",
        "def power_spectral_calc(data, arg_tuple):\n",
        "  return mne_features.univariate.compute_spect_entropy(arg_tuple,data)\n",
        "\n",
        "def univariate_window_calculation(operation, tensor, window_size,function, *var_args):\n",
        "  print(f'\\tExtracting Features for {operation}...',end='')\n",
        "  feature_agg = []\n",
        "  for sequence_id in range(len(tensor)):\n",
        "    print(f'Sequence {sequence_id}...',end='')\n",
        "    total_windows = int(tensor[sequence_id].shape[1]/window_size)\n",
        "    result = np.zeros((tensor[sequence_id].shape[0],total_windows))\n",
        "    if not var_args:\n",
        "      for i in range(total_windows):\n",
        "        left_end = i*window_size\n",
        "        right_end = left_end + window_size\n",
        "        result[:,i] = function(tensor[sequence_id][:,left_end:right_end])\n",
        "    else:\n",
        "      freq_list = var_args[0]\n",
        "      ind = min(sequence_id,len(freq_list)-1)\n",
        "      print(f'Sampling at {var_args[0][ind]}...',end='')\n",
        "      for i in range(total_windows):\n",
        "        left_end = i*window_size\n",
        "        right_end = left_end + window_size\n",
        "        result[:,i] = function(tensor[sequence_id][:,left_end:right_end],var_args[0][ind])\n",
        "    feature_agg.append(torch.from_numpy(result).transpose(0,1))\n",
        "  result_torch = torch.stack(feature_agg)\n",
        "  print(f'Shape={result_torch.shape}')\n",
        "  return result_torch\n",
        "\n",
        "def feature_extraction(tensor_1, sampling_freq, steps_per_points, key):\n",
        "  print(f'Extracting Features for {key}...',end='')\n",
        "  numpy_tensor = tensor_1.transpose(1,2).data.numpy()\n",
        "  print(f'Converted from torch to numpy; newshape={numpy_tensor.shape}')\n",
        "  feature1 = univariate_window_calculation('power spectral entropy', numpy_tensor, steps_per_points,power_spectral_calc,(sampling_freq))\n",
        "  feature2 = univariate_window_calculation('mean',numpy_tensor,steps_per_points,mne_features.univariate.compute_mean)\n",
        "  feature3 = univariate_window_calculation('zero crossings',numpy_tensor,steps_per_points,mne_features.univariate.compute_zero_crossings)\n",
        "  assert feature1.shape == feature2.shape and feature2.shape == feature3.shape\n",
        "  return [feature1, feature2, feature3]\n",
        "\n",
        "#good checkpoint to save the raw data -- will load data from there now on\n",
        "def feature_extraction_driver(current_directory,eeg_tensor, ecog_tensor, sampling_freq=[1000.0], steps_per_point=10, expected_len = 4,  save_checkpoint2=True):\n",
        "  eeg_all_data = [eeg_tensor]\n",
        "  ecog_all_data = [ecog_tensor]\n",
        "  ecog_all_data = ecog_all_data + feature_extraction(ecog_tensor,[1000.0],steps_per_point,'ECoG')\n",
        "  eeg_all_data = eeg_all_data + feature_extraction(eeg_tensor,sampling_freq,steps_per_point,\"EEG\")\n",
        "  assert len(eeg_all_data) == expected_len and len(ecog_all_data) == expected_len\n",
        "  if save_checkpoint2:\n",
        "    file_saving(current_directory,[\"mod_eeg_features_list.pt\",\"mod_ecog_features_list.pt\"],[eeg_all_data,ecog_all_data], dir_name=\"checkpoint2\")\n",
        "  return eeg_all_data"
      ],
      "metadata": {
        "id": "845WLpzVc6up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_full_data = feature_extraction_driver(\"/content/gdrive/MyDrive/BCI_Project/Datasets\",cleaned_eeg, cleaned_ecog, sampling_freq = frequency_tensors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Epwu-Hv_kb83",
        "outputId": "cbcc0aae-5841-4b11-9299-5e89987e6188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for ECoG...Converted from torch to numpy; newshape=(8, 129, 316689)\n",
            "\tExtracting Features for power spectral entropy...Sequence 0...Sampling at 1000.0...Sequence 1...Sampling at 1000.0...Sequence 2...Sampling at 1000.0...Sequence 3...Sampling at 1000.0...Sequence 4...Sampling at 1000.0...Sequence 5...Sampling at 1000.0...Sequence 6...Sampling at 1000.0...Sequence 7...Sampling at 1000.0...Shape=torch.Size([8, 31668, 129])\n",
            "\tExtracting Features for mean...Sequence 0...Sequence 1...Sequence 2...Sequence 3...Sequence 4...Sequence 5...Sequence 6...Sequence 7...Shape=torch.Size([8, 31668, 129])\n",
            "\tExtracting Features for zero crossings...Sequence 0...Sequence 1...Sequence 2...Sequence 3...Sequence 4...Sequence 5...Sequence 6...Sequence 7...Shape=torch.Size([8, 31668, 129])\n",
            "Extracting Features for EEG...Converted from torch to numpy; newshape=(8, 19, 316689)\n",
            "\tExtracting Features for power spectral entropy...Sequence 0...Sampling at 970.5352104083096...Sequence 1...Sampling at 972.9543618245476...Sequence 2...Sampling at 1065.819450806432...Sequence 3...Sampling at 991.5698696592785...Sequence 4...Sampling at 968.4353125735831...Sequence 5...Sampling at 955.7968617071556...Sequence 6...Sampling at 987.5395190279093...Sequence 7...Sampling at 984.1045149312752...Shape=torch.Size([8, 31668, 19])\n",
            "\tExtracting Features for mean...Sequence 0...Sequence 1...Sequence 2...Sequence 3...Sequence 4...Sequence 5...Sequence 6...Sequence 7...Shape=torch.Size([8, 31668, 19])\n",
            "\tExtracting Features for zero crossings...Sequence 0...Sequence 1...Sequence 2...Sequence 3...Sequence 4...Sequence 5...Sequence 6...Sequence 7...Shape=torch.Size([8, 31668, 19])\n",
            "Saving EEG and ECoG Tensors....\tFile mod_eeg_features_list.pt shape =[torch.Size([8, 316689, 19]), torch.Size([8, 31668, 19]), torch.Size([8, 31668, 19]), torch.Size([8, 31668, 19])]\n",
            "\tFile mod_ecog_features_list.pt shape =[torch.Size([8, 316689, 129]), torch.Size([8, 31668, 129]), torch.Size([8, 31668, 129]), torch.Size([8, 31668, 129])]\n",
            "Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Normalize Driver (Checkpt 3)- Normalizes numbers between (-1,1) and Splits data stored in files to train, test, and validation sets\n",
        "\n",
        "#normalizes a 3D tensor\n",
        "def normalize_data(data_tensor, key, feature, model_range=(-1,1),debug=True,transform = True):\n",
        "\n",
        "  scalers = []\n",
        "  normalized_data_tensor = [] if transform else None\n",
        "  numpy_data = data_tensor.data.numpy()\n",
        "  print(f'\\tNormalizing {key}\\'s {feature}: {numpy_data.shape}')\n",
        "\n",
        "  #create scaler for each sequence\n",
        "  for index in range(numpy_data.shape[0]):\n",
        "    scaler = MinMaxScaler(feature_range=model_range).fit(numpy_data[index])\n",
        "    scalers.append(scaler)\n",
        "    if debug:\n",
        "      print(f\"\\t\\tScaler Shapes=(Min={scaler.data_min_.shape};Max={scaler.data_max_.shape})\")\n",
        "    if transform == True:\n",
        "      normalized_data_tensor.append(torch.tensor(scaler.transform(numpy_data[index])))\n",
        "  finalized_tensor = torch.stack(normalized_data_tensor)\n",
        "  print(f'\\t\\tNormalized Shape={finalized_tensor.shape}')\n",
        "  return finalized_tensor, scalers\n",
        "\n",
        "def load_dataset(current_dir, dataset_dir, dataset_name, key, expected_size = 4):\n",
        "  feature_type = [\"data\",\"power\",\"mean\", \"zero\"]\n",
        "  dir_loc = os.path.join(current_dir,dataset_dir)\n",
        "  data_loc = None\n",
        "  if not os.path.exists(dir_loc):\n",
        "    raise Exception(f'Directory {dataset_dir} does not exist in {current_dir}\\ndownload from github again')\n",
        "  else:\n",
        "    data_loc = os.path.join(dir_loc,dataset_name)\n",
        "    if not os.path.exists(data_loc):\n",
        "      raise Exception(f'File {dataset_name} does not exist in {dir_loc}\\nDownload from github')\n",
        "  \n",
        "  tensor_data = torch.load(data_loc)\n",
        "  assert len(tensor_data) == expected_size\n",
        "  return tensor_data\n",
        "\n",
        "def normalize_dataset(current_dir,dataset_dir,key, tensor_data,norm_key,def_range = (-1,1),debug_top=True):\n",
        "  feature_type = [\"data\",\"power\",\"mean\", \"zero\"]\n",
        "  ecog_scalers = [] if \"ECoG\" in key else None\n",
        "  dir_loc = os.path.join(current_dir,dataset_dir)\n",
        "  normalized_data = []\n",
        "  print(f'Beginning to Normalize {key} Data in Range {def_range}; Key={norm_key}')\n",
        "  for dataset_id in range(len(tensor_data)):\n",
        "    if \"EEG\" in key:\n",
        "      norm_dataset, ___ = normalize_data(tensor_data[dataset_id],key,feature_type[dataset_id],model_range=def_range,debug=debug_top)\n",
        "    elif \"ECoG\" in key:\n",
        "      norm_dataset, scalers = normalize_data(tensor_data[dataset_id],key,feature_type[dataset_id],model_range=def_range,debug=debug_top)\n",
        "      ecog_scalers.append(scalers)\n",
        "    normalized_data.append(norm_dataset)\n",
        "  if ecog_scalers is not None:\n",
        "    print(f'Saving ECoG Scalers: Rows={len(ecog_scalers)}; Cols={len(ecog_scalers[0])}')\n",
        "    assert len(ecog_scalers) == 4\n",
        "    assert len(ecog_scalers[0]) == 8\n",
        "    joblib.dump(ecog_scalers,os.path.join(dir_loc,f\"ecog_feature_scalers_{norm_key}.save\"))\n",
        "  print(f'Finished Normalizing {key}')\n",
        "  return normalized_data\n",
        "\n",
        "def define_lengths(proportions, data_len, feature_len):\n",
        "  print(f'\\tFinding Splits for lengths {data_len} and {feature_len}...',end='')\n",
        "  feature_splits = [int(feature_len * prop) for prop in proportions]\n",
        "  data_splits = [int(data_len * prop) for prop in proportions]\n",
        "  partition_sizes = [data_len,feature_len]\n",
        "  split_nums = [data_splits,feature_splits]\n",
        "  for i in range(len(split_nums)):\n",
        "    summed_parts = sum(split_nums[i])\n",
        "    if summed_parts != partition_sizes[i]:\n",
        "      print(f'Adjusting for Length {i}...',end='')\n",
        "      split_nums[i][-1] = partition_sizes[i] - (summed_parts - split_nums[i][-1])\n",
        "      assert sum(split_nums[i]) == partition_sizes[i]\n",
        "  print('Determined Lengths: ')  \n",
        "  print(f'\\tLong Seq: {split_nums[0]}')\n",
        "  print(f'\\tShort Seq: {split_nums[1]}')\n",
        "  print('Finished')\n",
        "  return split_nums\n",
        "  \n",
        "def divide_tensors(tensor_list,splits, key, feature_set):\n",
        "  list_of_split_tensor = []\n",
        "  assert len(feature_set) == len(tensor_list)\n",
        "  for dataset_id in range(len(tensor_list)):\n",
        "    split_data = torch.split(tensor_list[dataset_id],splits[int(dataset_id > 0)],dim=1)\n",
        "    list_shapes = [elem.shape for elem in split_data]\n",
        "    print(f'\\tSplit {key} Tensor {feature_set[dataset_id]} into {list_shapes}')\n",
        "    list_of_split_tensor.append(split_data)\n",
        "  print('Finished')\n",
        "  return list_of_split_tensor\n",
        "\n",
        "def normalize_subdriver(current_directory,checkpt2_keys,load_file_names,checkpt3_keys,subdir_names,norm_key,my_range = (-1,1)):\n",
        "  features = [\"data\",\"power\", \"mean\", \"zero\"]\n",
        "  assert len(checkpt2_keys) == len(load_file_names)\n",
        "  for dataset_ind in range(len(checkpt2_keys)):\n",
        "    mod_list = load_dataset(current_directory,\"checkpoint2\",load_file_names[dataset_ind],checkpt2_keys[dataset_ind])\n",
        "    norm_list = normalize_dataset(current_directory,\"checkpoint2\",checkpt2_keys[dataset_ind],mod_list,norm_key,def_range=my_range,debug_top=False)\n",
        "    print(f'Dividing {checkpt2_keys[dataset_ind]} into Training,Validation, Test Sets')\n",
        "    split_lengths = define_lengths([0.7,0.2,0.1],norm_list[0].size(1), norm_list[1].size(1)) \n",
        "    tensor_list = [mod_list,norm_list]\n",
        "    for item_id in range(len(tensor_list)):\n",
        "      print(f'Saving {subdir_names[item_id]}...')\n",
        "      divided_list = divide_tensors(tensor_list[item_id],split_lengths,checkpt3_keys[item_id],features)\n",
        "      save_datasets(current_directory,[\"checkpoint3\",subdir_names[item_id]],features,divided_list,checkpt3_keys[item_id])\n",
        "  \n",
        "def normalize_driver(current_directory, dataset_ind=-1, def_range = (-1,1), checkpoint3 = True):\n",
        "  load_file_names = [\"mod_eeg_features_list.pt\",\"mod_ecog_features_list.pt\"]\n",
        "  keys = [\"EEG\",\"ECoG\"]\n",
        "  if def_range[0] == -1:\n",
        "    norm_key='11'\n",
        "  elif def_range[0] == 0:\n",
        "    norm_key='01'\n",
        "  if dataset_ind >= 0:\n",
        "    assert dataset_ind < 2\n",
        "    checkpt2_keys = [keys[dataset_ind]]\n",
        "    files_load = [load_file_names[dataset_ind]]\n",
        "    checkpt3_keys = [f\"{checkpt2_keys[0]}_Orig_{norm_key}\",f\"{checkpt2_keys[0]}_Norm_{norm_key}\"]\n",
        "    subdir_names = [f\"divided_{checkpt2_keys[0].lower()}_{norm_key}\",f\"normalized_{checkpt2_keys[0].lower()}_{norm_key}\"]\n",
        "  else:\n",
        "    checkpt2_keys = keys\n",
        "    files_load = load_file_names\n",
        "    checkpt3_keys = [f\"EEG_Orig_{norm_key}\",f\"ECoG_Orig_{norm_key}\",f\"EEG_Norm_{norm_key}\",f\"ECoG_Norm_{norm_key}\"]\n",
        "    subdir_names = [f\"divided_eeg_{norm_key}\",f\"divided_ecog_{norm_key}\",f\"normalized_eeg_{norm_key}\",f\"normalized_ecog_{norm_key}\"]\n",
        "  normalize_subdriver(current_directory,checkpt2_keys,files_load,checkpt3_keys,subdir_names,norm_key,my_range=def_range)"
      ],
      "metadata": {
        "id": "wTUVjJyBlNRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_driver(\"/content/gdrive/MyDrive/BCI_Project/Datasets\",dataset_ind=0,def_range=(0,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3NsBej4fKU8",
        "outputId": "a60db491-8b31-4a01-8445-d7355b4ee43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning to Normalize EEG Data in Range (0, 1); Key=01\n",
            "\tNormalizing EEG's data: (8, 316689, 19)\n",
            "\t\tNormalized Shape=torch.Size([8, 316689, 19])\n",
            "\tNormalizing EEG's power: (8, 31668, 19)\n",
            "\t\tNormalized Shape=torch.Size([8, 31668, 19])\n",
            "\tNormalizing EEG's mean: (8, 31668, 19)\n",
            "\t\tNormalized Shape=torch.Size([8, 31668, 19])\n",
            "\tNormalizing EEG's zero: (8, 31668, 19)\n",
            "\t\tNormalized Shape=torch.Size([8, 31668, 19])\n",
            "Finished Normalizing EEG\n",
            "Dividing EEG into Training,Validation, Test Sets\n",
            "\tFinding Splits for lengths 316689 and 31668...Adjusting for Length 0...Adjusting for Length 1...Determined Lengths: \n",
            "\tLong Seq: [221682, 63337, 31670]\n",
            "\tShort Seq: [22167, 6333, 3168]\n",
            "Finished\n",
            "Saving divided_eeg_01...\n",
            "\tSplit EEG_Orig_01 Tensor data into [torch.Size([8, 221682, 19]), torch.Size([8, 63337, 19]), torch.Size([8, 31670, 19])]\n",
            "\tSplit EEG_Orig_01 Tensor power into [torch.Size([8, 22167, 19]), torch.Size([8, 6333, 19]), torch.Size([8, 3168, 19])]\n",
            "\tSplit EEG_Orig_01 Tensor mean into [torch.Size([8, 22167, 19]), torch.Size([8, 6333, 19]), torch.Size([8, 3168, 19])]\n",
            "\tSplit EEG_Orig_01 Tensor zero into [torch.Size([8, 22167, 19]), torch.Size([8, 6333, 19]), torch.Size([8, 3168, 19])]\n",
            "Finished\n",
            "tensor_list[index] length = 3\n",
            "Saving EEG and ECoG Tensors....\tFile EEG_Orig_01_data_train.pt shape =torch.Size([8, 221682, 19])\n",
            "\tFile EEG_Orig_01_data_validation.pt shape =torch.Size([8, 63337, 19])\n",
            "\tFile EEG_Orig_01_data_test.pt shape =torch.Size([8, 31670, 19])\n",
            "Finished\n",
            "tensor_list[index] length = 3\n",
            "Saving EEG and ECoG Tensors....\tFile EEG_Orig_01_power_train.pt shape =torch.Size([8, 22167, 19])\n",
            "\tFile EEG_Orig_01_power_validation.pt shape =torch.Size([8, 6333, 19])\n",
            "\tFile EEG_Orig_01_power_test.pt shape =torch.Size([8, 3168, 19])\n",
            "Finished\n",
            "tensor_list[index] length = 3\n",
            "Saving EEG and ECoG Tensors....\tFile EEG_Orig_01_mean_train.pt shape =torch.Size([8, 22167, 19])\n",
            "\tFile EEG_Orig_01_mean_validation.pt shape =torch.Size([8, 6333, 19])\n",
            "\tFile EEG_Orig_01_mean_test.pt shape =torch.Size([8, 3168, 19])\n",
            "Finished\n",
            "tensor_list[index] length = 3\n",
            "Saving EEG and ECoG Tensors....\tFile EEG_Orig_01_zero_train.pt shape =torch.Size([8, 22167, 19])\n",
            "\tFile EEG_Orig_01_zero_validation.pt shape =torch.Size([8, 6333, 19])\n",
            "\tFile EEG_Orig_01_zero_test.pt shape =torch.Size([8, 3168, 19])\n",
            "Finished\n",
            "Saving normalized_eeg_01...\n",
            "\tSplit EEG_Norm_01 Tensor data into [torch.Size([8, 221682, 19]), torch.Size([8, 63337, 19]), torch.Size([8, 31670, 19])]\n",
            "\tSplit EEG_Norm_01 Tensor power into [torch.Size([8, 22167, 19]), torch.Size([8, 6333, 19]), torch.Size([8, 3168, 19])]\n",
            "\tSplit EEG_Norm_01 Tensor mean into [torch.Size([8, 22167, 19]), torch.Size([8, 6333, 19]), torch.Size([8, 3168, 19])]\n",
            "\tSplit EEG_Norm_01 Tensor zero into [torch.Size([8, 22167, 19]), torch.Size([8, 6333, 19]), torch.Size([8, 3168, 19])]\n",
            "Finished\n",
            "tensor_list[index] length = 3\n",
            "Saving EEG and ECoG Tensors....\tFile EEG_Norm_01_data_train.pt shape =torch.Size([8, 221682, 19])\n",
            "\tFile EEG_Norm_01_data_validation.pt shape =torch.Size([8, 63337, 19])\n",
            "\tFile EEG_Norm_01_data_test.pt shape =torch.Size([8, 31670, 19])\n",
            "Finished\n",
            "tensor_list[index] length = 3\n",
            "Saving EEG and ECoG Tensors....\tFile EEG_Norm_01_power_train.pt shape =torch.Size([8, 22167, 19])\n",
            "\tFile EEG_Norm_01_power_validation.pt shape =torch.Size([8, 6333, 19])\n",
            "\tFile EEG_Norm_01_power_test.pt shape =torch.Size([8, 3168, 19])\n",
            "Finished\n",
            "tensor_list[index] length = 3\n",
            "Saving EEG and ECoG Tensors....\tFile EEG_Norm_01_mean_train.pt shape =torch.Size([8, 22167, 19])\n",
            "\tFile EEG_Norm_01_mean_validation.pt shape =torch.Size([8, 6333, 19])\n",
            "\tFile EEG_Norm_01_mean_test.pt shape =torch.Size([8, 3168, 19])\n",
            "Finished\n",
            "tensor_list[index] length = 3\n",
            "Saving EEG and ECoG Tensors....\tFile EEG_Norm_01_zero_train.pt shape =torch.Size([8, 22167, 19])\n",
            "\tFile EEG_Norm_01_zero_validation.pt shape =torch.Size([8, 6333, 19])\n",
            "\tFile EEG_Norm_01_zero_test.pt shape =torch.Size([8, 3168, 19])\n",
            "Finished\n"
          ]
        }
      ]
    }
  ]
}